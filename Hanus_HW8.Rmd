---
title: "Homework 8"
author: "Nichole Hanus"
date: "Tuesday, March 24, 2016"
output: pdf_document
---

```{r, include=FALSE, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# Set the directory
setwd("~/2. Courses/36-608 Advanced Data Analysis/HW 8 (no HW 7)")

# Install required libraries and packages
install.packages("formula.tools", repos = "http://cran.us.r-project.org")
library(formula.tools) 

# for pretty tables
install.packages("knitr", repos = "http://cran.us.r-project.org")
library(knitr) 

# for kernel regression
install.packages("np",dependencies=TRUE, repos = "http://cran.us.r-project.org")
library(np) 

# Read stock data
stock <- load("~/2. Courses/36-608 Advanced Data Analysis/HW 8 (no HW 7)/stockData.RData")

```

# 1. Visualizing and transforming the data
## a. Visualize closing prices

```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

x <- as.Date(rownames(close_price))

# plot the closing prices for all stocks on the same graph
# use lines not points
matplot(as.Date(x), close_price,
        type = "l", ylim = c(0,300),
        ylab = "Closing Price ($)",
        xlab = "Date",
        main = "Closing Prices (01/15 - 12/15",
        axes = F)
axis(2)
axis.Date(1, x = as.Date(rownames(close_price)), las = 2)

```

We do see a clear dependence across the stocks (e.g. all stocks tend to follow roughly similar trends). We also see average closing prices ranging from less than $50 to greater than $200.

## b. Log daily transformation

```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# Create a new data frame with the log daily returns for each stock
# should have same number of columns and one few row

# create a blank dataframe
log_return <- data.frame(matrix(ncol=ncol(close_price), nrow=(nrow(close_price)-1)))

for (i in 1:ncol(log_return)){
  for (j in 1:nrow(log_return)){
    k <- j + 1
    log_return[j,i] <- log(close_price[k,i]/close_price[j,i])
  }
}
colnames(log_return) <- colnames(close_price)

```

## c. Recreate first plot using the log-transformed data

```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# plot the closing log prices for all stocks on the same graph
# use lines not points
matplot(as.Date(x[2:252]), log_return,
        type = "l",
        ylab = "Log Return ($)",
        xlab = "Date",
        main = "Log Returns (01/15 - 12/15)",
        axes = F)
axis(2)
axis.Date(1, x = as.Date(rownames(close_price)), las = 2)

```

The log returns are much closer in magnitude and range between roughly -0.1 and 0.1. Can more easily identify the variation.

# 2. Exploring the distribution of log returns.
## 2.a and 2.b Plot GE Log Returns and overlay a normal distribution curve

```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# plot GE log returns, column 11
hist(log_return[,11], 
     prob = TRUE,
     breaks = 30,
     xlab = "Log Returns",
     main = "Log Returns of GE",
     xlim = c(-0.05, .11))

# 2.b. Add the normal distribution curve
# curve() takes as its first parameter a function (or expression) that must be written as a function of x
# the x in dnorm() is not an object we created, but it indicates that a variable is being evaluated
# what is being evaluated is the normal density at the mean of y and standard deviation at y
curve(dnorm(x, 
            mean = mean(log_return[,11]),
            sd = sd(log_return[,11])),
      add = TRUE,
      col="blue")

```

The fitted normal distribution appears to fit the tails of the Log Returns of GE well, but doen't capture the peak in the histogram. We only see a part of the tail, but we know it is symmetric since it is Gaussian. Doesn't capture the very profitable days.



```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# 2.c.Use a kernal density estimate and perform CV to find the appropriate kernal bandwidth
# use the npudens function, which uses CV for the bw
# add the kernel regression gaussian line
plot(npudens(~log_return[,11]))
curve(dnorm(x, 
            mean = mean(log_return[,11]),
            sd = sd(log_return[,11])),
      add = TRUE,
      col="red")
# add a legend
legend('topright', c("Fitted Norm. Dist.", "Kernel Reg."), col = c("red","black"), lty=c(1,1))


```

The kernel regression curve (e.g. true density) is higher than the best fit Gaussian during periods of lower log returns (<-0.02) and when the log returns are higher than 0.03. The kernel regression fits the data better at the end tails (i.e. it increases a bit, similar to in the histogram).

## 2.d. Kernal density estimate for log returns

```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

## Plot kernel density estimates for all 28 stocks on the same plot

# create an empty plot
plot(1, type="n", ylab="Density", 
     xlab = "Log Return", 
     main = "Log Return across Stocks",
     xlim = c(-0.1, 0.1), ylim = c(0, 50))

# remove the multistart messages on output
options(np.messages = FALSE)

# loop through each of the stocks and perform kernal density function
# plot each kernel density
for(i in 1:(ncol(log_return))){
  
  kern <- npudensbw(~log_return[,i])
  stocks.dens <- npudens(kern)$dens
  
  order <- order(log_return[,i])
  x <- log_return[,i][order]
  y <- stocks.dens[order]
  
  lines(x, y, type="l", xlab = "", ylab = "", col = "blue")
}

```

Although we don't see much variability from these stocks, in terms of means, it is important to note that the timeframe is only for one year. We see some differences in mean (small differences from zero) and we see that some normal curves are slightly fatter than others. The ones with the high peaks suggest high density around zero and, therefore, more consistency. We might consider these stocks more stable over the observed time period than those stocks producing fatter Gaussian curves.

# 3. Principal components

```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# 3. compute the principal components of the returns
# use the prcomp function
# want analysis to center and scale the log returns
returns.pca = prcomp(log_return[,1:28], scale. = TRUE, center = TRUE, retx = TRUE)

# 3.a. make a barplot to report the weights of the first principal component
# first make a table for just the first component
PC1.weight <- returns.pca$rotation[,1]

#sort data to make it easier to plot
PC1.weight <- sort(PC1.weight)

#barchart 
barplot(PC1.weight, las = 2, ylim = c(0,0.3),
        xlab = "Stock Name",
        ylab = "Weight",
        main = "PC1")

```

We do not see a large range in weights (14% to 22%). We find the stocks with the greatest weight to be from banking (Goldman Sachs and JP Morgan). We also see computer technology-type stocks hanging together in the middle around 20% weight (Cisco Sysetms, IBM, and Verizon). Since all the weights seem very similar and do not have a very wide range, it suggests that they are all important and hang together. This finding complements the original log return graph across time, which shows dependencies across stocks.


```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# 3.b. plot projections against time
plot(as.Date(rownames(close_price)[2:252]), returns.pca$x[,1], type = "l",
     xlab = "Date",
     ylab = "PC1 Projection",
     main = "Projections across time (01/15 - 12/15)")

```

We see that the projections are behaving very similar to the aggregated log_return plot across time. There is variability across the timeline, but log_returns (or projections) are still centered around zero. 


```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}
# 3.c. plot the eigenvalues of the PCA
plot(returns.pca, type = "l", main = "")

```

It seems as if the first component explains the majority of the variance in the data set. The variance captured by each additonal PC drops off rapidly, suggesting that they are unnecessary. This scree plot suggests it is a good idea to use all the stocks and there isn't enough variance in their weights to conclude that one is less important than the other. The variance plotted here is along the principal components (along the x-axis). 


```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# 3.d. look at PC2
PC2.weight <- returns.pca$rotation[,2]

#sort data to make it easier to plot
PC2.weight <- sort(PC2.weight)

#barchart 
barplot(PC2.weight, las = 2,
        xlab = "Stock Name",
        ylab = "Weight",
        main = "PC2")

```

This second PC captures the direction of the largest variance from the one-dimensional model. We see that the most positive weights (>0.3) include stocks from the agricultural and oil industries. We find the most negative values (<0.2) to be related to retail and home improvement (e.g. sectors that the general public can immediately participate in).

# 4. Fit a one-factor model.

```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# fit a one-factor model (set tol =1 meaning that components are omiited
# if their standard deviations are less than or equal to 1 times the SD of PC1)
returns.fa = factanal(log_return, factors = 1, scores = "regression")

# plot the loadings
fa.loadings <- as.vector(returns.fa$loadings)
names(fa.loadings) <- rownames(returns.fa$loadings)
fa.loadings <- sort(fa.loadings)

#barchart 
barplot(fa.loadings , las = 2,
        xlab = "Stock Name",
        ylab = "Loadings")

# compare loadings to first PC weights
plot(scale(fa.loadings), scale(PC1.weight),
     xlab = "Loadings",
     ylab = "PC1 Weights")
abline(0,1)

```

In the barchart of the loadings for the one-factor model, we see an identical trend to the first principal component weights. However, the scale is now much larger (0 to 1). The scale for the weights of the first principal component were smaller (ranging from roughly 0.1 to 0.25).

We then plot the loadings against the first PC weights and find a nearly perfect 1-to-1 relationship.


```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# plot the factor score against the date
plot(as.Date(rownames(close_price)[2:252]), returns.fa$scores, type = "l",
     xlab = "Date",
     ylab = "Loadings",
     main = "Loadings across time (01/15 - 12/15)")

```

We see that the loadings are, again, behaving very similar to the aggregated log_return plot across time. There is variability across the timeline, but log_returns (or projections) are still centered around zero. This plot looks similar to the PC1 projections; however, the loadings scale is smaller (-4 to 4) than the projections scale (-15 to 15).

# 5. Use bootstrapping to provide 90% CIs on the factor loadings

```{r, warning=FALSE, message=FALSE, cache=TRUE, fig.align='center'}

# resampler function
return.resampler <- function() {
  n <- nrow(log_return)
  resample.rows <- sample(1:n,size=n,replace=TRUE)
  return(log_return[resample.rows,])
}

# estimator function
est.fa <- function(data) {
    returns.fa.sim = factanal(data, factors = 1, scores = "regression")
    loadings <- returns.fa.sim$loadings
    return(loadings)
}


# test stat
test <- as.vector(returns.fa$loadings)

# develop CIs from resampled cases
loadings.cis <- function(B, alpha, existing.load = test){
  cases.boots <- replicate(B, est.fa(return.resampler()))
  cis.lower <- 2*existing.load - apply(cases.boots, 1, 
                                            quantile, probs = 1-alpha/2)
  cis.upper <- 2*existing.load - apply(cases.boots, 1, 
                                            quantile, probs = alpha/2)
  return(list(lower.ci=cis.lower,upper.ci=cis.upper))
  }

fa.loadings.ci <- as.data.frame(loadings.cis(B=1000, alpha = 0.05, 
                                       existing.load = test))

# first need to add the CIs to the fa.loadings
# then order them
fa.loadings2 <- as.vector(returns.fa$loadings)
names(fa.loadings2) <- rownames(returns.fa$loadings)

# add CIs
fa.loadings2 <- as.data.frame(cbind(fa.loadings2, fa.loadings.ci))
fa.loadings2 <- fa.loadings2[order(fa.loadings2[,1]),]

# plot this in the previous barplot of loadings
par(mar = c(5, 6, 4, 5) + 0.1)

barCenters <- barplot(height = fa.loadings,
                  names.arg = names(fa.loadings),
                  beside = true, las = 2,
                  cex.names = 0.75,
                  main = "Factor Loadings",
                  ylab = "Loadings",
                  xlab = "Stock",
                  border = "black", axes = TRUE, las = 2,
                  ylim = c(0,1.3))

# Specify the groupings. We use srt = 45 for a
# 45 degree string rotation
segments(barCenters, fa.loadings2[,2], barCenters,
         fa.loadings2[,3], lwd = 1.5)

arrows(barCenters, fa.loadings2[,2], barCenters,
       fa.loadings2[,3], lwd = 1.5, angle = 90,
       code = 3, length = 0.05)
# add a legend
legend('topleft', "90% CI", col = "black", lty=1)

```

Finally, we plot the 90% CI's on the barchart of the one-factor model loadings. We find Dupont (DD), United Health Group (UNH), and 3M (MMM) have the highest variability within the observed dataset. It is unclear why this might be, without knowing more about how certain sectors perform in the stock market. This plot demonstrates 90% confidence intervals changing outcomes of the factor analysis, if the extreme tails were considered. 

